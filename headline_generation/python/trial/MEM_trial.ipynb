{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.370\n",
      "             2          -0.40504        0.802\n",
      "             3          -0.34871        0.818\n",
      "             4          -0.32351        0.824\n",
      "             5          -0.30940        0.827\n",
      "             6          -0.30039        0.831\n",
      "             7          -0.29412        0.833\n",
      "             8          -0.28949        0.833\n",
      "             9          -0.28591        0.833\n",
      "            10          -0.28306        0.834\n",
      "            11          -0.28073        0.834\n",
      "            12          -0.27878        0.834\n",
      "            13          -0.27712        0.836\n",
      "            14          -0.27570        0.836\n",
      "            15          -0.27446        0.837\n",
      "            16          -0.27338        0.837\n",
      "            17          -0.27241        0.837\n",
      "            18          -0.27156        0.837\n",
      "            19          -0.27079        0.838\n",
      "            20          -0.27009        0.838\n",
      "            21          -0.26947        0.837\n",
      "            22          -0.26889        0.837\n",
      "            23          -0.26837        0.837\n",
      "            24          -0.26789        0.838\n",
      "            25          -0.26744        0.838\n",
      "            26          -0.26703        0.838\n",
      "            27          -0.26665        0.838\n",
      "            28          -0.26630        0.838\n",
      "            29          -0.26596        0.838\n",
      "            30          -0.26565        0.838\n",
      "            31          -0.26536        0.838\n",
      "            32          -0.26509        0.838\n",
      "            33          -0.26483        0.838\n",
      "            34          -0.26459        0.838\n",
      "            35          -0.26436        0.838\n",
      "            36          -0.26415        0.838\n",
      "            37          -0.26394        0.838\n",
      "            38          -0.26374        0.838\n",
      "            39          -0.26356        0.838\n",
      "            40          -0.26338        0.838\n",
      "            41          -0.26321        0.838\n",
      "            42          -0.26305        0.838\n",
      "            43          -0.26290        0.838\n",
      "            44          -0.26275        0.838\n",
      "            45          -0.26261        0.838\n",
      "            46          -0.26247        0.838\n",
      "            47          -0.26234        0.838\n",
      "            48          -0.26221        0.838\n",
      "            49          -0.26209        0.838\n",
      "            50          -0.26198        0.837\n",
      "            51          -0.26186        0.837\n",
      "            52          -0.26175        0.837\n",
      "            53          -0.26165        0.837\n",
      "            54          -0.26155        0.837\n",
      "            55          -0.26145        0.837\n",
      "            56          -0.26136        0.837\n",
      "            57          -0.26126        0.837\n",
      "            58          -0.26118        0.837\n",
      "            59          -0.26109        0.837\n",
      "            60          -0.26101        0.837\n",
      "            61          -0.26092        0.837\n",
      "            62          -0.26085        0.837\n",
      "            63          -0.26077        0.838\n",
      "            64          -0.26070        0.838\n",
      "            65          -0.26062        0.838\n",
      "            66          -0.26055        0.838\n",
      "            67          -0.26049        0.838\n",
      "            68          -0.26042        0.838\n",
      "            69          -0.26035        0.838\n",
      "            70          -0.26029        0.838\n",
      "            71          -0.26023        0.838\n",
      "            72          -0.26017        0.838\n",
      "            73          -0.26011        0.838\n",
      "            74          -0.26005        0.838\n",
      "            75          -0.26000        0.838\n",
      "            76          -0.25995        0.838\n",
      "            77          -0.25989        0.838\n",
      "            78          -0.25984        0.838\n",
      "            79          -0.25979        0.838\n",
      "            80          -0.25974        0.838\n",
      "            81          -0.25969        0.838\n",
      "            82          -0.25965        0.838\n",
      "            83          -0.25960        0.838\n",
      "            84          -0.25955        0.838\n",
      "            85          -0.25951        0.838\n",
      "            86          -0.25947        0.838\n",
      "            87          -0.25943        0.838\n",
      "            88          -0.25938        0.838\n",
      "            89          -0.25934        0.838\n",
      "            90          -0.25930        0.837\n",
      "            91          -0.25927        0.837\n",
      "            92          -0.25923        0.837\n",
      "            93          -0.25919        0.837\n",
      "            94          -0.25915        0.837\n",
      "            95          -0.25912        0.837\n",
      "            96          -0.25908        0.837\n",
      "            97          -0.25905        0.837\n",
      "            98          -0.25901        0.837\n",
      "            99          -0.25898        0.837\n",
      "         Final          -0.25895        0.837\n"
     ]
    }
   ],
   "source": [
    "# attempt at using nltk MEM \n",
    "# based on http://textminingonline.com/tag/maxent-classifier\n",
    "import random\n",
    "from nltk.corpus import names\n",
    "from nltk import MaxentClassifier\n",
    "\n",
    "all_names = [(name, 'male') for name in names.words('male.txt')]\n",
    "female_names = [(name, 'female') for name in names.words('female.txt')]\n",
    "\n",
    "all_names.extend(female_names)\n",
    "random.shuffle(all_names) # shuffle names\n",
    "\n",
    "def gender_features(word):\n",
    "    # returns a dictionary of feature sets\n",
    "    word = word.lower()\n",
    "    feature_dict = {'last_letter': word[-1]}\n",
    "    feature_dict['first_letter'] = word[0]\n",
    "    feature_dict['fw'] = word[:2]\n",
    "    feature_dict['lw'] = word[-2:]\n",
    "    \n",
    "    return feature_dict\n",
    "\n",
    "\n",
    "feature_sets = [(gender_features(name), gender) for (name, gender) in all_names]\n",
    "\n",
    "train_set, test_set = feature_sets[100:], feature_sets[:100]\n",
    "\n",
    "classifier = MaxentClassifier.train(train_set)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import classify\n",
    "\n",
    "classify.accuracy(classifier, test_set)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
